# MwWn-20190417

目前的方向大致上嘗試往變異（variation）和組合（composition）嘗試。這兩個方向可能都會和四字格是否是一個單位（unitized, established, adhesive）有關。在這個研究中，我們試圖找出「是單位」的四字格，並在WordNet中用synset代表它的意義。

## 一、QIE的使用變異（variation）

這部分試圖從語料庫中尋找特定四字格的變異用法，例如「相互扶持」（僅是舉例，這個例子不見得是我們想找的QIE），在語料庫中就有「相扶持」、「相互支持」、「相互依賴扶持」、「相互鼓勵扶持」等用法。

原本這階段遇到的問題是計算需要的時間太長。以原本的方法計算13億字中30萬個四字格的變異頻率，大概會需要2,747個小時（4個processes同時進行）。原本演算法的問題是重複密集地迭代過所有語料，每個四字格要計算變異頻率時，都需要走過全部的語料，非常沒有效率。變異資料對之後的研究可能會有幫助，所以為了減少運算時間，我首先試著在搜尋變異頻率之前，先建立文章層次的字索引。這樣真正搜尋時可以直接透過索引，找到那些有可能是四字格變異出現的文章。其次，雖然當初找到的四字格有30萬個，但我們可能僅需要一部份的資料即可。我後來僅使用出現頻率最高的82,317個四字格（出現322次以上，詳細決定方式請見[ipynb][select_ngrams]）。這兩個方法大幅降低計算變異所需的時間：在所有的QIE語料（包括中時、蘋果和PTT，約13億字）找完約8萬個QIE的變異約需7小時。

[select_ngrams]: https://github.com/seantyh/MWE2019/blob/master/etc/select_ngrams.ipynb

每個四字格的變異都有5個次數，分別是在第二字位置替代、刪除，在第三字位置替代、刪除，以及在第一二、二三、三四字之間的插入（在任一個位置中，最多插入10個字）。除了記錄變異次數外，同時也把實際的變異事例（instance）記錄下來。如果接下來需要的話，這些變異事例可能會有其他訊息，例如，替代是否都是替代同一個字，甚至插入是否都是同一個位子的特定字等等。

相關資料在Google Drive：

```
G2.NLP.NLU/ACL-MWE/data/variations/qievars_[corpus].csv        變異次數（corpus: apple|chinatimes|ptt）
G2.NLP.NLU/ACL-MWE/data/variations/[corpus]_variations.json.gz 變異事例
```

## 二、描述QIE的組合性（compositionality）

四字格的組合性應該和透明度緊密關連，「相互扶持」是透明（transparent）的四字格，因為四字格的意義和其所包含的四個字的字義是相關的；相對地，「對牛彈琴」的意義，除了四個字的字面意義外，該四字格的意義是隱喻延伸而得的，所以此四字格較不透明（opqaue）。

為了描述一個四字格的透明與否，我們想建立一套能描述組合性的計算方法。一個很直覺的方式是從四字格的構成字著手：我們可以先找出「字向量」，並計算每個「字向量」之間的分數，例如餘弦近似分數（cosine similarity），並把這些分數整合起來，建立出一個「四字格的分數」。或許該分數會和此四字格的組合性有關。

然而，「字向量」並不是一個容易的東西，畢竟一般計算語意向量模型的層次不是「字」，而是「詞」。如果直接引用「詞向量」模型中的單字詞當作「字向量」的表徵，結果會非常不直覺，例如以下是在詞向量（FastText, wiki-zh pretrained）模型中，「養」的鄰近詞：

```
[('認', 0.9780995845794678),
 ('產', 0.9743243455886841),
 ('說', 0.9737245440483093),
 ('還', 0.9735196232795715),
 ('當', 0.9732926487922668)]
```

這是「兒」的鄰近詞：

```
[('潔', 0.973732054233551),
 ('與', 0.9711471796035767),
 ('凱', 0.9703478813171387),
 ('絲', 0.9701326489448547),
 ('蓮', 0.9700840711593628)]
```

這些例子反應的可能是這些單字詞在「詞向量」模型中，僅能學習到這些「字」被當作「單字詞」使用時的語意關係，如果「養」當作單字詞時的鄰近詞很多樣，那「詞向量」模型也很難穩定地找出與之相連的詞。

為了建立較能反映字意義的向量表徵，我從構詞網絡著手。構詞網路內包含CWN中的單字詞和雙字詞，這些單字詞與雙字詞之間的關係，來自於（1）CWN的語意關係（包括上下位、近義、反義、同義等關係），以及（2）字（單字詞）與詞的構詞關係。事實上，這些詞也都同時有詞向量資料（Fasttext pretrained的資料，但目前未使用到這些訊息）。目前這個網絡共有18,251個節點，27,932個關係。

接下來，我試圖建立節點向量表徵（node embeddings），亦即在構詞網絡中，找出這18,251個節點的向量表徵。在這步驟中想嘗試節點向量的原因是來自於一個猜想：如果一個四字格很透明，也就是其組合性很低，那四字格中的字與字關係，應該就很像一般拿來構詞的關係。所以如果我們能用使從構詞網絡中建立節點向量，且該節點向量能告訴我們一個四字格的組合，有多類似一般詞彙的構詞關係，那我們就能藉此描述組合性。過去研究有數種建立節點向量的模型。有些模型較偏重監督式（supervised）學習，例如預測圖形中節點的類別（node labeling），或預測節點與節點之間是否有連結（link prediction）。這些模型會藉由節點的屬性向量和連結關係，建立數層的節點向量表徵，並透過深度學習模型做出預測。例如GraphSage, Graph Convolutional Network(GCN)。但我們的用途大致上是非監督式的，亦即我們想要找出這些節點的某種表徵，並藉由它們來計算節點與節點（特別是字與字）之間的關係。過去文獻中，這類模型包含node2vec或DeepWalk。它們都和word2vec相似，是完全的非監督學習（unsupervised learning），但是node2vec的優點是比DeepWalk多了可控制節點的「脈絡」（neighbor）。所以接下來我使用的是node2vec（Grover & Leskovec, 2016）。

我用node2vec的不同參數，建立了三種節點向量，分別是（1）同質關係（homophily）、（2）隨機路徑（random walk）、（3）偏重結構（structure equivalence）。事實上，這三種模型看起來結果相似。以下是三種模型對「養」和「兒」的例子（僅呈現鄰近的單字詞）。

```
# show_nv_samples('養')

--- Homophily ---
[('餵', 0.8972288966178894),
 ('畜', 0.8379783034324646),
 ('飼', 0.8221665620803833),
 ('蓄', 0.8183460831642151),
 ('撫', 0.7352026104927063)]
--- Random Walk ---
[('餵', 0.905147910118103),
 ('蓄', 0.8543225526809692),
 ('畜', 0.8442349433898926),
 ('飼', 0.7797045707702637),
 ('撫', 0.7777304649353027)]
--- Structure Equivalence ---
[('畜', 0.9135547876358032),
 ('餵', 0.912987232208252),
 ('蓄', 0.8916459083557129),
 ('飼', 0.8151147365570068),
 ('撫', 0.7542983889579773)]
```

```
# show_nv_samples('兒')

--- Homophily ---
[('鳳', 0.9601813554763794),
 ('男', 0.9254202842712402),
 ('娃', 0.9033308625221252),
 ('犢', 0.8762232065200806),
 ('仔', 0.8657761216163635)]
--- Random Walk ---
[('鳳', 0.9728346467018127),
 ('男', 0.9188355207443237),
 ('女', 0.875070333480835),
 ('犢', 0.8745706081390381),
 ('娃', 0.8691387176513672)]
--- Structure Equivalence ---
[('鳳', 0.9629033803939819),
 ('男', 0.9488214254379272),
 ('犢', 0.9092534780502319),
 ('娃', 0.8866438269615173),
 ('仔', 0.8721786737442017)]
```

從這些例子看起來，構詞網絡雖然只有WordNet的語意關係和簡單的構詞關係，但透過這些連結所計算出來的字向量，「直觀上」好像比詞向量模型提供的鄰近詞有意義。然而，從這些結果而言，雖然構詞網絡的關係是構詞，但字向量空間中反應的，還是字義相近程度。所以如果我們要用這些字向量來反應的是構詞關係，好像並不直接。

以下是一種從節點向量計算字連結分數的方法：

$$
p(x_2 \mid x_1) = \textrm{softmax}(\phi(x_1, x_2); x_2) =
    \frac{\exp\left(\phi(x_1, x_2)\right)}
    {\sum_{y \in X} \exp\left(\phi(x_1, y)\right)}
$$

$\phi(x, y)$ 是兩節點間的餘弦近似（cosine similarity）分數。四字格的組合性機率分數，即從上述式子延伸而得。此分數將假設四字格只有相鄰的兩字相依，所有其他的字都是在機率上獨立的。亦即：

$$
p(x_1, x_2, x_3, x_4) = p(x_2 \mid x_1)\,p(x_3 \mid x_2)\,p(x_4 \mid x_3)
$$

按照上述關係，可計算以下四字格的組合性機率分數：

```
---- 養兒防老 ----
homophily:  -25.15175
Random Walk:  -25.02118
Struct Equiv:  -24.845314
> values shown are log probabilities
```

```
---- 相互扶持 ----
homophily:  -23.825315
Random Walk:  -23.805883
Struct Equiv:  -23.889053
> values shown are log probabilities
```

```
---- 對牛彈琴 ----
homophily:  -24.727793
Random Walk:  -24.793005
Struct Equiv:  -24.709614
> values shown are log probabilities
```

從這些例子來看，或許四字格的組合性機率分數有捕捉到一點點「組合性」：「相互扶持」的組合性的確高於「對牛彈琴」。但是「對牛彈琴」是具備衍生意義的成語，其組合性應低於「養兒防老」（雖是固定用法，但沒有成語性的衍生意義），但其組合性分數卻是「對牛彈琴」較高。所以，從構詞網絡建立「字向量」，並以之預測組合性，或許只是抓到了部份訊息。

## Notebooks

- [cwn_vectors](https://github.com/seantyh/MWE2019/blob/master/etc/cwn_vectors.ipynb)
  找出CWN的單字詞、雙字詞，並在FastText找出相對應的word embeddings，並建立詞彙索引。`CwnMorphGraph` 和 `CwnNodeVec` 的詞彙索引都是來字於 `CwnVectors`。
- [cwn_adjacency](https://github.com/seantyh/MWE2019/blob/master/etc/cwn_adjacency.ipynb)
  建立Morphology graph，這個圖事後來所有node embeddings所用的資料來源
- [cwn_node_vec](https://github.com/seantyh/MWE2019/blob/master/etc/cwn_node_vec.ipynb)
  用node2vec建立node embedding，並嘗試不同random walk參數，計算最近似字，以及計算連結機率
